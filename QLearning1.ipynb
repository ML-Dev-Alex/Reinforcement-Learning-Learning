{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters, change these for possibly better results \n",
    "n_states = 40\n",
    "max_iterations = 10000\n",
    "lr_init = 1.0\n",
    "lr_min = 0.003\n",
    "gamma = 1\n",
    "t_max = 10000\n",
    "epsilon = 0.02\n",
    "\n",
    "env_name = 'MountainCar-v0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    reward_total = 0\n",
    "    step_index = 0\n",
    "    for _ in range(t_max):\n",
    "        if render:\n",
    "            env.render()\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            a, b = obs_to_state(env, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_total += gamma ** step_index * reward\n",
    "        step_index += 1\n",
    "        if done:\n",
    "            break\n",
    "    return reward_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R function, where the algorithm learns from its observations\n",
    "def obs_to_state(env, obs):\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make(env_name)\n",
    "    q_table = np.zeros((n_states, n_states, 3))\n",
    "    for i in range(max_iterations):\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        # adjust learning rate\n",
    "        eta = max(lr_min, lr_init * (0.85 ** (i//100)))\n",
    "        for j in range(t_max):\n",
    "            a, b = obs_to_state(env, obs)\n",
    "            #take a random step sometimes so we don't get stuck on local min\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                # create a proper probability function out of the learned table and use it to take the proper action \n",
    "                logits = q_table[a][b]\n",
    "                logits_exp = np.exp(logits)\n",
    "                prob = logits_exp/np.sum(logits_exp)\n",
    "                action = np.random.choice(env.action_space.n, p=prob)\n",
    "                \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            reward_total += reward\n",
    "            a_, b_ = obs_to_state(env, obs)\n",
    "            # bellman equation\n",
    "            # update the q table with what we learned from last action and predicted values\n",
    "            q_table[a][b][action] = q_table[a][b][action]\\\n",
    "                                                + eta * (reward + gamma * np.max(q_table[a_][b_])\n",
    "                                                         - q_table[a][b][action])\n",
    "            if done:\n",
    "                break\n",
    "        if i % 100 == 0:\n",
    "            print('Iteration #%d -- Total reward %d.' % (i+1, reward_total))\n",
    "    \n",
    "    # select best learned actions to use on final try        \n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "    print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "    \n",
    "    run_episode(env, solution_policy, True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
